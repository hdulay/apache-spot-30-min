
#
# Copyright (c) 2014 Cloudera, Inc. All rights reserved.
#

#
# Simple AWS Cloudera Director configuration file with automatic role assignments
# that works as expected if you use a single instance type for all cluster nodes
#

#
# Cluster name
#

name: apache-spot-secured

#
# Cloud provider configuration (credentials, region or zone and optional default image)
#

provider {
    type: aws

    #
    # Get AWS credentials from the OS environment
    # See http://docs.aws.amazon.com/general/latest/gr/aws-security-credentials.html
    #
    # If specifying the access keys directly and not through variables, make sure to enclose
    # them in double quotes.
    #
    # Leave the accessKeyId and secretAccessKey fields blank when running on an instance
    # launched with an IAM role.

    accessKeyId: ${?AWS_ACCESS_KEY_ID}
    secretAccessKey: ${?AWS_SECRET_ACCESS_KEY}

    #
    # ID of the Amazon AWS region to use
    # See: http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-regions-availability-zones.html
    #

    region: ${?aws_region}

    #
    # Region endpoint (if you are using one of the Gov. regions)
    #

    # regionEndpoint: ec2.us-gov-west-1.amazonaws.com

    #
    # ID of the VPC subnet
    # See: http://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC_Subnets.html
    #

    subnetId: ${?aws_subnet}

    #
    # Comma separated list of security group IDs
    # See: http://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC_SecurityGroups.html
    #

    securityGroupsIds: ${?aws_security_group}

    #
    # A prefix that Cloudera Director should use when naming the instances (this is not part of the hostname)
    #

    instanceNamePrefix: spot-cluster-secured

    #
    # Specify a size for the root volume (in GBs). Cloudera Director will automatically expand the
    # filesystem so that you can use all the available disk space for your application
    # See: http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/storage_expand_partition.html
    #

    # rootVolumeSizeGB: 100 # defaults to 50 GB if not specified

    #
    # Specify the type of the EBS volume used for the root partition. Defaults to gp2
    # See: http://aws.amazon.com/ebs/details/
    #

    # rootVolumeType: gp2 # OR standard (for EBS magnetic)

    #
    # Whether to associate a public IP address with instances or not. If this is false
    # we expect instances to be able to access the internet using a NAT instance
    #
    # Currently the only way to get optimal S3 data transfer performance is to assign
    # public IP addresses to your instances and not use NAT (public subnet type of setup)
    #
    # See: http://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/vpc-ip-addressing.html
    #

    # associatePublicIpAddresses: true

}

#
# SSH credentials to use to connect to the instances
#

ssh {
    username: ec2-user # for RHEL image
    privateKey: ${?path_to_private_key} # with an absolute path to .pem file
}

#
# A list of instance types to use for group of nodes or management services
#


common-instance-properties {
    # hvm        |  RHEL-7.2_HVM_GA-20151112-x86_64-1-Hourly2-GP2        |  ami-2051294a - original, ami-85241def
    # hvm        |  RHEL-7.3_HVM_GA-20161026-x86_64-1-Hourly2-GP2        |  ami-b63769a1
    # hvm        |  RHEL-7.3_HVM-20170613-x86_64-4-Hourly2-GP2           |  ami-9e2f0988
    # ami-6d1c2007 - centos72
    image: ami-85241def
    tags {
        owner: ${?aws_owner}
    }
    # this script needs to already be downloaded on the director host
    bootstrapScriptPath: "./java8.sh"
}

instances {

    m42x : ${common-instance-properties}  {
        type: m4.2xlarge
    }

    m44x : ${common-instance-properties} {
        type: m4.4xlarge
    }

    m4xl : ${common-instance-properties} {
        type: m4.xlarge
    }

    t2l : ${common-instance-properties} {   # only suitable as a gateway
        type: t2.large
    }

    t2xl : ${common-instance-properties} {   # only suitable as a gateway
        type: t2.xlarge
    }
}

#
# Configuration for Cloudera Manager. Cloudera Director can use an existing instance
# or bootstrap everything from scratch for a new cluster
#

cloudera-manager {

    instance: ${instances.m42x} {
        tags {
            application: "CM5"
            group: cm
        }
    }

    csds: [
        "http://archive.cloudera.com/spark2/csd/SPARK2_ON_YARN-2.2.0.cloudera2.jar",
    ]

    javaInstallationStrategy: NONE

    #
    # Automatically activate 60-Day Cloudera Enterprise Trial
    #

    enableEnterpriseTrial: true

    unlimitedJce: true

    # Kerberos principal and password for use by Cloudera Director 
    krbAdminUsername: ${?krbAdminUsername}
    krbAdminPassword: ${?krbAdminPassword}

    # Cloudera Manager configuration values  
    configs {
        CLOUDERA_MANAGER {
            KDC_TYPE: ${?KDC_TYPE}
            KDC_HOST: ${?KDC_HOST}
            SECURITY_REALM: ${?SECURITY_REALM}
            KRB_MANAGE_KRB5_CONF: true
            KRB_ENC_TYPES: ${?KRB_ENC_TYPES}
        }
    }  

}

#
# Cluster description
#

cluster {

    # List the products and their versions that need to be installed.
    # These products must have a corresponding parcel in the parcelRepositories
    # configured above. The specified version will be used to find a suitable
    # parcel. Specifying a version that points to more than one parcel among
    # those available will result in a configuration error. Specify more granular
    # versions to avoid conflicts.

    products {
        CDH: 5.12
        SPARK2: 2.2
        KAFKA: 2
        KUDU: 1.3.0
    }

    parcelRepositories: ["http://archive.cloudera.com/cdh5/parcels/5.12/",
                         "http://archive.cloudera.com/kafka/parcels/2.2/",
                         "http://archive.cloudera.com/spark2/parcels/2.2.0/",
                         "https://repo.continuum.io/pkgs/misc/parcels/",
                         "http://archive.cloudera.com/kudu/parcels/5.12.0/"]

    services: [HDFS, YARN, ZOOKEEPER, HIVE, IMPALA, KAFKA, SPARK2_ON_YARN, HUE, OOZIE, SENTRY, HBASE, KUDU]

    masters {
        count: 1
        instance: ${instances.m44x} {
            tags {
                group: master
            }
        }

        roles {
            HDFS: [NAMENODE, SECONDARYNAMENODE]
            YARN: [RESOURCEMANAGER, JOBHISTORY]
            ZOOKEEPER: [SERVER]
            HIVE: [HIVESERVER2, HIVEMETASTORE]
            IMPALA: [CATALOGSERVER, STATESTORE]
            SPARK2_ON_YARN: [SPARK2_YARN_HISTORY_SERVER]
            HUE: [HUE_SERVER]
            OOZIE: [OOZIE_SERVER]
            KAFKA: [KAFKA_BROKER]
            SENTRY: [SENTRY_SERVER]
            HBASE: [MASTER]
            HBASE: [HBASETHRIFTSERVER]
            KUDU: [KUDU_MASTER]
        }

        configs {
            KUDU {
                KUDU_MASTER {
                   # The master rarely performs IO. If fs_data_dirs is unset, it will
                   # use the same directory as fs_wal_dir
                   fs_wal_dir: "/data0/kudu/masterwal"
                   fs_data_dirs: "/data1/kudu/master"
                }
            }
      }
    }

    workers {
        count: 3
        minCount: 3
        instance: ${instances.m42x} {
            tags {
                group: worker
            }
        }
        roles {
            HDFS: [DATANODE]
            YARN: [NODEMANAGER]
            IMPALA: [IMPALAD]
            HBASE: [REGIONSERVER]
            KUDU: [KUDU_TSERVER]
        }
        configs {
            KUDU {
                KUDU_TSERVER {
                    # Set fs_wal_dir to an SSD drive (if exists) for better performance.
                    # Set fs_data_dirs to a comma-separated string containing all remaining
                    # disk drives, solid state or otherwise.
                    # If there are multiple drives in the machine, it's best to ensure that
                    # the WAL directory is not located on the same disk as a tserver data
                    # directory.
                    fs_wal_dir: "/data0/kudu/tabletwal"
                    fs_data_dirs: "/data1/kudu/tablet"
                }
            }
      }
    }

    gateways {
        count: 1

        instance: ${instances.t2l} {
            tags {
                group: gateway
            }
        }

        roles {
            HIVE: [GATEWAY]
            SPARK2_ON_YARN: [GATEWAY]
            HDFS: [GATEWAY]
            YARN: [GATEWAY]
            HBASE: [GATEWAY]
        }
    }
    
}
